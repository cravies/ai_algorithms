{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zXVxB6bM1_mh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ebd9e7b-e366-4db4-f0b6-b200e41b9360"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from keras.datasets import mnist\n",
        "from random import sample\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "SCALE_FACTOR = 255\n",
        "WIDTH = x_train.shape[1]\n",
        "HEIGHT = x_train.shape[2]\n",
        "x_train = x_train.reshape(x_train.shape[0],WIDTH*HEIGHT).T / SCALE_FACTOR\n",
        "x_test = x_test.reshape(x_test.shape[0],WIDTH*HEIGHT).T  / SCALE_FACTOR"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mnist from scratch\n",
        "\n",
        "* fast vectorised implementation using matrix operations\n",
        "* simple 2 layer MLP\n",
        "* 1 hidden layer size 10, reLU activation\n",
        "* 1 output layer, size 10, softmax activation\n",
        "* based on tutorial at https://www.youtube.com/watch?v=w8yWXqWQYmU\n",
        "* I added stochastic gradient descent by randomly sampling rows, is a massive speedup on convergence"
      ],
      "metadata": {
        "id": "ga1PBSzj7rrz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# global run settings\n",
        "ETA = 0.01\n",
        "EPOCHS=1000\n",
        "SAMPLES=1000\n",
        "#initialise input\n",
        "A_0 = np.asarray([np.reshape(np.asarray(x),[784,]) for x in x_train.T]).T\n",
        "print(A_0.shape)\n",
        "#initalise weights\n",
        "W_1 = np.random.randn(10,784) * np.sqrt(1./(784))\n",
        "b_1 = np.random.randn(10,1) * np.sqrt(1./(10))\n",
        "W_2 = np.random.randn(10,10) * np.sqrt(1./(20))\n",
        "b_2 = np.random.randn(10,1) * np.sqrt(1./(784))\n",
        "np.mean(W_1) #should be ~~0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IOmfjHe7BaG",
        "outputId": "79b23539-b1dd-486a-c2e9-ebd41838d230"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(784, 60000)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0004056875217560816"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def reLU(x):\n",
        "    return np.maximum(x,0)\n",
        "\n",
        "def softmax(Z):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    exp = np.exp(Z - np.max(Z))\n",
        "    return exp / exp.sum(axis=0)\n",
        "\n",
        "def reLU_diff(x):\n",
        "    return x > 0\n",
        "\n",
        "# forward propagate\n",
        "def forward_prop(A_0, W_1, b_1, W_2, b_2):\n",
        "    z_1 = W_1.dot(A_0) + b_1\n",
        "    # relu\n",
        "    A_1 = reLU(z_1)\n",
        "    z_2 = W_2.dot(A_1) + b_2\n",
        "    # apply softmax\n",
        "    A_2 = softmax(z_2)\n",
        "    return z_1, A_1, z_2, A_2\n",
        "\n",
        "def one_hot_Y(classnum):\n",
        "    # one hot vector encoding for MNIST class labels\n",
        "    res = np.zeros(10)\n",
        "    res[classnum] = 1\n",
        "    return res.T\n",
        "\n",
        "def back_prop(A_0,A_1,A_2,z_1,Y,W_1,b_1,W_2,b_2):\n",
        "    samp_num = Y.shape[1] #number of training samples\n",
        "    # ~~~~~~~~~ dC/dW_2 ~~~~~~~~~~~~\n",
        "    dz_2 = 2*(A_2 - Y)\n",
        "    dW_2 = 1/(samp_num) * dz_2.dot(A_1.T)\n",
        "    # ~~~~~~~~~ dC/db_2 ~~~~~~~~~~~~\n",
        "    db_2 = 1/(samp_num) * np.sum(dz_2,1).reshape([10,1])\n",
        "    # ~~~~~~~~~ dC/dW_1 ~~~~~~~~~~~~\n",
        "    dz_1 = W_2.T.dot(dz_2) * reLU_diff(z_1) \n",
        "    dW_1 = 1/(samp_num) * dz_1.dot(A_0.T)\n",
        "    # ~~~~~~~~~ dC/db_1 ~~~~~~~~~~~~\n",
        "    db_1 = 1/(samp_num) * np.sum(dz_1,1).reshape([10,1])\n",
        "    W_1 -= ETA * dW_1\n",
        "    W_2 -= ETA * dW_2\n",
        "    b_1 -= ETA * db_1\n",
        "    b_2 -= ETA * db_2\n",
        "    return W_1, b_1, W_2, b_2\n",
        "\n",
        "def return_acc(A_2, Y):\n",
        "    #want to iterate over cols\n",
        "    A_2 = A_2.T\n",
        "    Y = Y.T\n",
        "    correct = 0\n",
        "    for i in range(len(A_2)):\n",
        "        if np.argmax(A_2[i])==np.argmax(Y[i]):\n",
        "            correct += 1\n",
        "    return (correct / len(A_2))\n",
        "\n",
        "def stochastic_gradient_descent(A_0, Y, W_1, b_1, W_2, b_2, EPOCHS=EPOCHS):\n",
        "    # let's perform stochastic gradient descent\n",
        "    Y = np.asarray([one_hot_Y(y) for y in Y]).T\n",
        "    for i in range(EPOCHS):\n",
        "        # randomly sample a subset of examples\n",
        "        idx = np.random.choice(A_0.shape[1], size=SAMPLES, replace=False)\n",
        "        A_0_sample = A_0[:,idx]\n",
        "        Y_sample = Y[:,idx]\n",
        "        z_1, A_1, z_2, A_2 = forward_prop(A_0_sample, W_1, b_1, W_2, b_2)\n",
        "        W_1, b_1, W_2, b_2 = back_prop(A_0_sample, A_1, A_2, z_1, \n",
        "                                        Y_sample, W_1, b_1, W_2, b_2)\n",
        "        acc = return_acc(A_2,Y_sample)\n",
        "        print(f\"Epoch {i} training accuracy: {acc}\")\n",
        "    return W_1, b_1, W_2, b_2"
      ],
      "metadata": {
        "id": "QZbOg7k365TC"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train using SGD\n",
        "W_1, b_1, W_2, b_2 = stochastic_gradient_descent(A_0, y_train, W_1, b_1, W_2, b_2, 10)\n",
        "# lets do some testing\n",
        "input = np.asarray([np.reshape(np.asarray(x),[784,]) for x in x_test.T]).T\n",
        "labels = np.asarray([one_hot_Y(y) for y in y_test]).T\n",
        "_,_,_,preds = forward_prop(input,W_1,b_1,W_2,b_2)\n",
        "acc = return_acc(preds,labels)\n",
        "print(\"~\"*30)\n",
        "print(\"Testing Accuracy: \",acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zc8JLLw3uXv1",
        "outputId": "8b560681-2d86-434f-f603-f1465e3aeeaa"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 training accuracy: 0.495\n",
            "Epoch 1 training accuracy: 0.476\n",
            "Epoch 2 training accuracy: 0.488\n",
            "Epoch 3 training accuracy: 0.45\n",
            "Epoch 4 training accuracy: 0.462\n",
            "Epoch 5 training accuracy: 0.497\n",
            "Epoch 6 training accuracy: 0.458\n",
            "Epoch 7 training accuracy: 0.495\n",
            "Epoch 8 training accuracy: 0.506\n",
            "Epoch 9 training accuracy: 0.466\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Testing Accuracy:  0.4901\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nice."
      ],
      "metadata": {
        "id": "-FBfQwxxvVdD"
      }
    }
  ]
}